{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아이펠톤에서 사용할 데이터 생성하는 노트북\n",
    "\n",
    "### 발생한 이슈\n",
    "- 같은 텍스트를 넣었는데, openai api에서 포함해야 하는 텍스트를 제외하는 일이 발생함 -> 문장유사도를 판단하여, 유사한 문장을 포함하지 않으면 제외하는 방식 사용\n",
    "\n",
    "### 코드 흐름\n",
    "- 데이터를 한줄씩 가져옴\n",
    "- 포함해야 하는 컬럼의 데이터를 openai api 에 넣어 문장을 생성함\n",
    "- 포함해야 하는 컬럼의 데이터를 문장별로 나눔\n",
    "- 생성된 문장 내에 '포함해야 하는 문장'들이 존재하는지 문장유사도를 사용하여 확인\n",
    "- 문장유사도 비율이 낮으면 중단하고 저장, 원인 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from difflib import SequenceMatcher\n",
    "import winsound\n",
    "import ftfy\n",
    "import re\n",
    "import string\n",
    "import glob\n",
    "\n",
    "# NLTK 패키지에서 Punkt tokenizer를 다운로드 (한 번만 실행)\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"API_KEY\")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 부호를 제거한 후, 소문자로 변환하고 단어로 분리하는 함수\n",
    "def clean_and_split(sentence):\n",
    "    # 문장 부호 제거 (string.punctuation을 사용하여 기본적인 문장 부호 제거)\n",
    "    cleaned_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 소문자로 변환하고 단어로 분리\n",
    "    words = cleaned_sentence.lower().split()\n",
    "    \n",
    "    return words\n",
    "\n",
    "# 두 문장 간의 포함 비율을 계산하는 함수\n",
    "# base 문장이 story에 포함되어 있는 경우 \n",
    "def partial_inclusion_ratio(base_sentence, sentence):\n",
    "    # base_sentence와 sentence의 길이를 기준으로 공통 문자를 확인\n",
    "    base_words = list(clean_and_split(base_sentence))\n",
    "    sentence_words = list(clean_and_split(sentence))\n",
    "\n",
    "    # 공통 문자를 찾기 위해 base_sentence의 문자들이 sentence에 얼마나 포함되는지 확인\n",
    "    common_words = [word for word in base_words if word in sentence_words]\n",
    "    \n",
    "    # 포함된 문자 비율을 계산\n",
    "    if len(base_words) == 0:\n",
    "        inclusion_ratio = 0\n",
    "    else:\n",
    "        inclusion_ratio = len(common_words) / len(base_words)\n",
    "    \n",
    "    return inclusion_ratio\n",
    "\n",
    "# 문장유사도 비교\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# '포함해야 하는 문장들' 내의 1개의 문장과 '만들어진 장문' 내 문장들을 비교하여, 가장 유사한 문장을 1개 반환 \n",
    "def find_similar_sentences(base_sentence, long_text, threshold=0):\n",
    "    # 긴 텍스트를 문장으로 분리\n",
    "    sentences = nltk.sent_tokenize(long_text)\n",
    "    \n",
    "    # 유사한 문장 추출\n",
    "    base = \"\"\n",
    "    long = \"\"\n",
    "    similarity_final = 0\n",
    "    include_rate = 0.7\n",
    "    for sentence in sentences:\n",
    "        # 포함 비율 계산\n",
    "        inclusion_ratio = partial_inclusion_ratio(base_sentence, sentence)\n",
    "        \n",
    "        # base 문장이 stroy 문장에 부분 포함관계에 있는지 확인\n",
    "        if inclusion_ratio >= include_rate:\n",
    "            similarity = inclusion_ratio*100  # 유사도를 1 이상으로 설정\n",
    "        else:\n",
    "            similarity = similar(base_sentence, sentence)  # 포함되지 않으면 유사도 계산\n",
    "        \n",
    "        # 유사도 소수점 셋째 자리까지 반올림\n",
    "        similarity = round(similarity, 3)\n",
    "\n",
    "        if similarity >= threshold and similarity > similarity_final:\n",
    "            base = base_sentence\n",
    "            long = sentence\n",
    "            similarity_final = similarity\n",
    "    return base, long, similarity_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api함수 \n",
    "def get_chatgpt_response_c2d2(input_01, input_02):\n",
    "    # OpenAI API를 통해 ChatGPT에게 한국어로 자연스럽게 다듬어 달라고 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        # model=\"gpt-4o-mini\",\n",
    "        # model =\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a patient receiving psychological counseling. who can speak english only.\"},\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": f\"다음에 제공되는 2 문장은 너가 쓴 일기인데, 2~3문장이 빠져 있어 {input_01} {input_02}, 주어진 문장을 그대로 변경 없이 포함해서, 비어있는 내용을 영어로 써줘\"}\n",
    "        ]\n",
    "    )\n",
    "    # return response['choices'][0]['message']['content']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 아래 데이터를 생성함\n",
    "- annotated data (캐글의 심리학자-환자 대화 셋)의 전체 장문, 상담 받는 사람의 생각, 해당 생각의 인지왜곡 type 과 포멧을 맞춤\n",
    "- (중국어 -> 영어로 변역된) C2D2 데이터셋의 시나리오+thought (인지 왜곡된 생각)을 맞춰 장문을 만들어냄\n",
    "- 생성하며, 문장유사도가 낮은 문장이 포함된다면 생성을 멈출 것 -> 전체 데이터가 잘 못 생성되는 경우가 있음\n",
    "\n",
    "\n",
    "\n",
    "- 선행연구 C2d2 dataset: A resource for the cognitive distortion analysis and its impact on mental health. 에서 사용된 데이터셋임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Num</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Thought</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm an introverted person, and I've just arriv...</td>\n",
       "      <td>Are the people in this environment unfriendly?</td>\n",
       "      <td>Overgeneralization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Recently, I feel dizzy sometimes when I stand ...</td>\n",
       "      <td>I'm so dizzy. Am I sick? I should probably go ...</td>\n",
       "      <td>No Distortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I'm walking down the street and feel hungry, b...</td>\n",
       "      <td>I'm tired and there's no place to rest, I'm hu...</td>\n",
       "      <td>Overgeneralization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Work has been busy lately, but I have caught a...</td>\n",
       "      <td>Why did I catch a cold at this time? I feel so...</td>\n",
       "      <td>No Distortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>My mom and I are discussing future plans. She ...</td>\n",
       "      <td>Mom is trying to control my life again, wantin...</td>\n",
       "      <td>Fortune-telling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Num                                           Scenario  \\\n",
       "0    1  I'm an introverted person, and I've just arriv...   \n",
       "1    2  Recently, I feel dizzy sometimes when I stand ...   \n",
       "2    3  I'm walking down the street and feel hungry, b...   \n",
       "3    4  Work has been busy lately, but I have caught a...   \n",
       "4    5  My mom and I are discussing future plans. She ...   \n",
       "\n",
       "                                             Thought               Label  \n",
       "0     Are the people in this environment unfriendly?  Overgeneralization  \n",
       "1  I'm so dizzy. Am I sick? I should probably go ...       No Distortion  \n",
       "2  I'm tired and there's no place to rest, I'm hu...  Overgeneralization  \n",
       "3  Why did I catch a cold at this time? I feel so...       No Distortion  \n",
       "4  Mom is trying to control my life again, wantin...     Fortune-telling  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = pd.read_csv(\"raw_data/Final_Modified_C2D20911.csv\")\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Num                                                                1025\n",
       "Scenario              I discover that I am not my parents' biologica...\n",
       "Thought               I’m sorry, I’m sorry. I’m obviously not my par...\n",
       "Label                                                   Personalization\n",
       "Scenario_Sentences    [I discover that I am not my parents' biologic...\n",
       "Thought_Sentences     [I’m sorry., I’m sorry., I’m obviously not my ...\n",
       "all_Sentences         [I discover that I am not my parents' biologic...\n",
       "Name: 1024, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.iloc[1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [I'm an introverted person., and I've just arr...\n",
       "1       [Recently., I feel dizzy sometimes when I stan...\n",
       "2       [I'm walking down the street and feel hungry.,...\n",
       "3       [Work has been busy lately., but I have caught...\n",
       "4       [My mom and I am discussing future plans., She...\n",
       "                              ...                        \n",
       "7498    [I want to be tall., rich and handsome., but I...\n",
       "7499    [I feel that I have poor expressive skills and...\n",
       "7500    [Frequent overtime work makes I feel too uncom...\n",
       "7501    [I feel that I have poor expressive skills and...\n",
       "7502    [I woke up in the morning and wanted to sleep ...\n",
       "Name: all_Sentences, Length: 7503, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 번역 잘못된 부분 처리\n",
    "# 'Ir'을 'my'로 변경\n",
    "raw['Scenario'] = raw['Scenario'].str.replace(' Ir ', ' my ', case=False)\n",
    "raw['Scenario'] = raw['Scenario'].str.replace(' i, ', ' me, ', case=False)\n",
    "raw['Scenario'] = raw['Scenario'].str.replace('I are ', 'I am ', case=False)\n",
    "raw['Scenario'] = raw['Scenario'].str.replace('i are ', 'i am ', case=False)\n",
    "\n",
    "raw['Thought'] = raw['Thought'].str.replace(' Ir ', ' my ', case=False)\n",
    "raw['Thought'] = raw['Thought'].str.replace(' i, ', ' me, ', case=False)\n",
    "raw['Thought'] = raw['Thought'].str.replace('I are ', 'I am ', case=False)\n",
    "raw['Thought'] = raw['Thought'].str.replace('i are ', 'i am ', case=False)\n",
    "\n",
    "\n",
    "# 포함해야 하는 데이터들을 문장 단위로 쪼갬\n",
    "# , 가 있는 문장을 생성할떄 .으로 바뀌며 두개로 나뉠 수 있으므로 변경함\n",
    "raw['Scenario_Sentences'] = raw['Scenario'].str.replace(', ', '. ', case=False)\n",
    "raw['Thought_Sentences'] = raw['Thought'].str.replace(', ', '. ', case=False)\n",
    "\n",
    "raw['Scenario_Sentences'] = raw['Scenario_Sentences'].apply(lambda x: nltk.sent_tokenize(x))\n",
    "raw['Thought_Sentences'] = raw['Thought_Sentences'].apply(lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "raw['all_Sentences'] = raw['Scenario_Sentences'] + raw['Thought_Sentences']\n",
    "raw['all_Sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한개만 실행해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm an introverted person, and I've just arrived in a new environment where the people around I are mostly unfamiliar. This change has made me feel quite anxious and unsure about how to proceed. I find myself worrying whether I will be able to connect with anyone here. Are the people in this environment unfriendly? Sometimes, I think it might be my own fears holding me back from trying.\n"
     ]
    }
   ],
   "source": [
    "# 한개만 실행해봄\n",
    "index_num = 0\n",
    "\n",
    "refined_response = get_chatgpt_response_c2d2(raw[\"Scenario\"][index_num], raw[\"Thought\"][index_num])\n",
    "\n",
    "# 결과 출력\n",
    "print(refined_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 저장할 데이터프레임 생성\n",
    "result_df = pd.DataFrame()\n",
    "    \n",
    "base_list = []\n",
    "long_list = []\n",
    "similarity_list = []\n",
    "\n",
    "long_text = refined_response.choices[0].message.content\n",
    "\n",
    "for base_text in raw.iloc[index_num]['all_Sentences']:\n",
    "    # print(base_text)\n",
    "    base, long, similarity = find_similar_sentences(base_text, long_text)\n",
    "\n",
    "    base_list.append(base)\n",
    "    long_list.append(long)\n",
    "    similarity_list.append(similarity)\n",
    "\n",
    "# 유사도가 있는 경우 평균 계산, 없으면 0으로 설정\n",
    "if similarity_list:\n",
    "    average_similarity = sum(similarity_list) / len(similarity_list)\n",
    "else:\n",
    "    average_similarity = 0\n",
    "    \n",
    "# 데이터프레임으로 만들어서 비교\n",
    "result = pd.DataFrame({\n",
    "    'base': [base_list],\n",
    "    'long': [long_list],\n",
    "    'similarity': [similarity_list],\n",
    "    'average_similarity': average_similarity\n",
    "})\n",
    "\n",
    "result_df = pd.concat([result_df, result], ignore_index=True)\n",
    "\n",
    "# 저장\n",
    "result_df.to_csv(\"data/re.csv\", index=False)\n",
    "# result_df.to_csv(file_path_list, mode='a', header=False, index=False)  # 이후 100개씩 추가\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 전체 데이터 반복 실행\n",
    "코드 구조\n",
    "- 문장 생성\n",
    "- cleaning (chatgpt가 가끔 의견을 덧붙힐 때가 있음 - 그걸 제거함)\n",
    "- 유사도 비교, 데이터 프레임으로 만들어서 합침\n",
    "- 유사도가 낮은 문장이 나오거나, 루프가 끝나면 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "       Num                                           Scenario  \\\n",
      "7405  7406  Suddenly I'm walking outside and realize my cl...   \n",
      "\n",
      "                                                Thought          Label  \\\n",
      "7405  It's over. This is so embarrassing. I wanted t...  No Distortion   \n",
      "\n",
      "                                     Scenario_Sentences  \\\n",
      "7405  [Suddenly I'm walking outside and realize my c...   \n",
      "\n",
      "                                      Thought_Sentences  \\\n",
      "7405  [It's over., This is so embarrassing., I wante...   \n",
      "\n",
      "                                          all_Sentences  \n",
      "7405  [Suddenly I'm walking outside and realize my c...  \n"
     ]
    }
   ],
   "source": [
    "start_num = 7405 # 쓸꺼면 엑셀 숫자 다음꺼\n",
    "\n",
    "raw_filtered = raw.iloc[start_num:]\n",
    "print(len(raw_filtered))\n",
    "print(raw_filtered.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 반복문\n",
    "result_df = pd.DataFrame() # 결과 저장할 데이터프레임 생성\n",
    "end_num = len(raw_filtered)\n",
    "\n",
    "for i, row in raw_filtered.iterrows():\n",
    "    # print(row)\n",
    "    \n",
    "    # 문장 생성\n",
    "    refined_response = get_chatgpt_response_c2d2(row[\"Scenario\"], row[\"Thought\"])\n",
    "\n",
    "    # 생성된 문장 확인 \n",
    "    gan_sentence = refined_response.choices[0].message.content\n",
    "    # print(gan_sentence)\n",
    "\n",
    "    # 문장 필터링\n",
    "    # story = clean_refined_thought(gan_sentence)\n",
    "    # print(story)\n",
    "\n",
    "    # 유사도 비교\n",
    "    base_list = []\n",
    "    long_list = []\n",
    "    similarity_list = []\n",
    "\n",
    "    for base_text in row['all_Sentences']:\n",
    "        # print(base_text)\n",
    "        base, long, similarity = find_similar_sentences(base_text, gan_sentence)\n",
    "\n",
    "        base_list.append(base)\n",
    "        long_list.append(long)\n",
    "        similarity_list.append(similarity)\n",
    "\n",
    "    # 유사도가 있는 경우 평균 계산, 없으면 0으로 설정\n",
    "    if similarity_list:\n",
    "        average_similarity = sum(similarity_list) / len(similarity_list)\n",
    "    else:\n",
    "        average_similarity = 0\n",
    "        \n",
    "    # 데이터프레임으로 만들어서 비교\n",
    "    result = pd.DataFrame({\n",
    "        'num' : row[\"Num\"],\n",
    "        'scenario' : row[\"Scenario\"],\n",
    "        'thought' : row[\"Thought\"],\n",
    "        'label' : row[\"Label\"],\n",
    "        'gen_sentence' : gan_sentence,\n",
    "        # 'story' : story,\n",
    "        'base': [base_list],\n",
    "        'long': [long_list],\n",
    "        'similarity': [similarity_list],\n",
    "        'average_similarity': average_similarity\n",
    "    })\n",
    "\n",
    "    result_df = pd.concat([result_df, result], ignore_index=True)\n",
    "\n",
    "    # 유사도가 일정 이하거나, 전체 데이터를 다 돌은 경우, 저장하고 루프 종료\n",
    "    has_below = any(num < 0.6 for num in similarity_list)\n",
    "\n",
    "    if has_below or i == end_num-1:\n",
    "        print(f\"유사도가 0.6 미만인 경우 발생: {row['Num']}\")\n",
    "\n",
    "        # 비프음 출력해서 알림\n",
    "        # 주파수(Hz)와 지속 시간(ms) 설정\n",
    "        frequency = 1000  # 주파수: 1000Hz\n",
    "        duration = 5000    # 지속 시간: 500ms\n",
    "\n",
    "        # 비프음 출력\n",
    "        winsound.Beep(frequency, duration)\n",
    "        \n",
    "        # 정제 후 저장\n",
    "        result_df['Cleaned'] = result_df['gen_sentence'].apply(clean_refined_thought)\n",
    "        result_df.to_csv(f\"data/gen_c2d2/c2d2_{start_num}_{i}.csv\", index=False)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['Cleaned'] = result_df['gen_sentence'].apply(clean_refined_thought)\n",
    "result_df.to_csv(f\"data/gen_c2d2/c2d2_{start_num}_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, I can help fill in the missing sentences while keeping the provided text unchanged:\n",
      "\n",
      "I woke up in the morning and wanted to sleep a little longer, but when I woke up I found that I was late. I rushed to get ready and skipped breakfast to make up for lost time. Why didn't my roommates call me? They must have wanted to see me being scolded for being late. They must have been laughing at me secretly and wanted to embarrass me. I couldn't believe they would do that to me. It made me feel so alone and betrayed.\n",
      "[100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "100.0\n",
      "['I woke up in the morning and wanted to sleep a little longer.', 'but when I woke up I found that I was late.', \"Why didn't my roommates call me?\", 'They must have wanted to see me being scolded for being late.', 'They must have been laughing at me secretly and wanted to embarrass me.']\n",
      "['Sure, I can help fill in the missing sentences while keeping the provided text unchanged:\\n\\nI woke up in the morning and wanted to sleep a little longer, but when I woke up I found that I was late.', 'Sure, I can help fill in the missing sentences while keeping the provided text unchanged:\\n\\nI woke up in the morning and wanted to sleep a little longer, but when I woke up I found that I was late.', \"Why didn't my roommates call me?\", 'They must have wanted to see me being scolded for being late.', 'They must have been laughing at me secretly and wanted to embarrass me.']\n"
     ]
    }
   ],
   "source": [
    "print(result_df.iloc[-1].gen_sentence)\n",
    "print(result_df.iloc[-1].similarity)\n",
    "print(result_df.iloc[-1].average_similarity)\n",
    "print(result_df.iloc[-1].base)\n",
    "print(result_df.iloc[-1].long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 합치기\n",
    "- 따로따로 만들어진 데이터를 합침"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더 경로 설정 (예: 'your_folder_path')\n",
    "folder_path = 'data/gen_c2d2'\n",
    "\n",
    "# 해당 폴더 내 모든 CSV 파일 경로 가져오기\n",
    "all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# 파일들을 저장할 리스트 생성\n",
    "df_list = []\n",
    "\n",
    "# 모든 CSV 파일을 불러와서 리스트에 추가\n",
    "for file in all_files:\n",
    "    df = pd.read_csv(file, encoding='ISO-8859-1')  # CSV 파일 읽기\n",
    "    df_list.append(df)\n",
    "\n",
    "# 리스트에 있는 모든 데이터프레임을 아래로 합치기\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 합쳐진 데이터를 새로운 CSV 파일로 저장\n",
    "merged_df.to_csv('data/c2d2_after_gen_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 정제\n",
    "- gpt 붙힌 사족 제거\n",
    "- encoding문제로 깨져 보이는 것, 문법적으로 틀린 곳 교정 (눈에 보이는 것만 일단 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num                                           scenario  \\\n",
      "0  1.0  I'm an introverted person, and I've just arriv...   \n",
      "\n",
      "                                          thought               label  \\\n",
      "0  Are the people in this environment unfriendly?  Overgeneralization   \n",
      "\n",
      "                                        gen_sentence  \\\n",
      "0  Sure, I can help with that:\\n\\n\"I'm an introve...   \n",
      "\n",
      "                                                base  \\\n",
      "0  [\"I'm an introverted person.\", \"and I've just ...   \n",
      "\n",
      "                                                long             similarity  \\\n",
      "0  ['Sure, I can help with that:\\n\\n\"I\\'m an intr...  [100.0, 100.0, 100.0]   \n",
      "\n",
      "  average_similarity                                            Cleaned  ...  \\\n",
      "0              100.0  I'm an introverted person, and I've just arriv...  ...   \n",
      "\n",
      "  Unnamed: 16 Unnamed: 17 Unnamed: 18 Unnamed: 19 Unnamed: 20  Unnamed: 21  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN          NaN   \n",
      "\n",
      "  Unnamed: 22 Unnamed: 23 Unnamed: 24 Unnamed: 25  \n",
      "0         NaN         NaN         NaN         NaN  \n",
      "\n",
      "[1 rows x 26 columns]\n",
      "7501\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "c2d2_gen_all = pd.read_csv(\"data/c2d2_after_gen_all.csv\")\n",
    "print(c2d2_gen_all.head(1))\n",
    "print(len(c2d2_gen_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num                                                               253.0\n",
      "scenario              I find out that I didn?t save the PPT I worke...\n",
      "thought               I worked on it for so long, but because I didn...\n",
      "label                                               Emotional Reasoning\n",
      "gen_sentence          Sure, I can help with that. Here?s the comple...\n",
      "base                                      'I worked on it for so long.'\n",
      "long                                  \"\"but because I didn't save it.\"\"\n",
      "similarity                                        'everything is gone.'\n",
      "average_similarity                                 'I feel really bad.'\n",
      "Cleaned                                    'So why did I work so hard?'\n",
      "Unnamed: 10                                                   'Anyway.'\n",
      "Unnamed: 11                                       'the time is wasted.'\n",
      "Unnamed: 12            'so I might as well go to sleep and just do s...\n",
      "Unnamed: 13           ['Here?s the completion of your diary entry:\\...\n",
      "Unnamed: 14           [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100...\n",
      "Unnamed: 15                                                       100.0\n",
      "Unnamed: 16           ?I find out that I didn?t save the PPT I wor...\n",
      "Unnamed: 17                                only sending text messages.\"\n",
      "Unnamed: 18           We may not be in touch with each other as fami...\n",
      "Unnamed: 19                                               No Distortion\n",
      "Unnamed: 20           Sure, let me fill in the gaps for you.\\n\\n---\\...\n",
      "Unnamed: 21           ['I have been in contact with my family less o...\n",
      "Unnamed: 22           ['I have been in contact with my family less o...\n",
      "Unnamed: 23                                [100.0, 100.0, 100.0, 100.0]\n",
      "Unnamed: 24                                                       100.0\n",
      "Unnamed: 25           Lately, I've felt somewhat disconnected from m...\n",
      "Name: 1728, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scenario</th>\n",
       "      <th>thought</th>\n",
       "      <th>label</th>\n",
       "      <th>gen_sentence</th>\n",
       "      <th>base</th>\n",
       "      <th>long</th>\n",
       "      <th>similarity</th>\n",
       "      <th>average_similarity</th>\n",
       "      <th>Cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm an introverted person, and I've just arriv...</td>\n",
       "      <td>Are the people in this environment unfriendly?</td>\n",
       "      <td>Overgeneralization</td>\n",
       "      <td>Sure, I can help with that:\\n\\n\"I'm an introve...</td>\n",
       "      <td>[\"I'm an introverted person.\", \"and I've just ...</td>\n",
       "      <td>['Sure, I can help with that:\\n\\n\"I\\'m an intr...</td>\n",
       "      <td>[100.0, 100.0, 100.0]</td>\n",
       "      <td>100.0</td>\n",
       "      <td>I'm an introverted person, and I've just arriv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            scenario  \\\n",
       "0  I'm an introverted person, and I've just arriv...   \n",
       "\n",
       "                                          thought               label  \\\n",
       "0  Are the people in this environment unfriendly?  Overgeneralization   \n",
       "\n",
       "                                        gen_sentence  \\\n",
       "0  Sure, I can help with that:\\n\\n\"I'm an introve...   \n",
       "\n",
       "                                                base  \\\n",
       "0  [\"I'm an introverted person.\", \"and I've just ...   \n",
       "\n",
       "                                                long             similarity  \\\n",
       "0  ['Sure, I can help with that:\\n\\n\"I\\'m an intr...  [100.0, 100.0, 100.0]   \n",
       "\n",
       "  average_similarity                                            Cleaned  \n",
       "0              100.0  I'm an introverted person, and I've just arriv...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 잘못 합쳐진 데이터 제거\n",
    "# c2d2_gen_all[c2d2_gen_all['Unnamed: 25'].notna()]\n",
    "print(c2d2_gen_all.iloc[1728])\n",
    "data = c2d2_gen_all.drop([1728])\n",
    "data = data[['scenario', 'thought', 'label', 'gen_sentence', 'base', 'long', 'similarity', 'average_similarity', 'Cleaned']]\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7499"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터가 없는 row 제거\n",
    "data = data.dropna(subset=['gen_sentence'])\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding문제로 깨져 보이는 것 수정\n",
    "# ftfy를 사용해 컬럼의 모든 텍스트의 인코딩 문제를 수정\n",
    "data['gen_sentence'] = data['gen_sentence'].apply(ftfy.fix_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       I'm an introverted person, and I've just arriv...\n",
      "1       Recently, I feel dizzy sometimes when I stand ...\n",
      "2       I'm walking down the street and feel hungry, b...\n",
      "3       Work has been busy lately, but I have caught a...\n",
      "4       My mom and I am discussing future plans. She m...\n",
      "                              ...                        \n",
      "7496    I want to be tall, rich and handsome, but I do...\n",
      "7497    I feel that I have poor expressive skills and ...\n",
      "7498    Certainly. Here's the continuation of your dia...\n",
      "7499    I feel that I have poor expressive skills and ...\n",
      "7500    I woke up in the morning and wanted to sleep a...\n",
      "Name: Cleaned, Length: 7499, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 함수 정의: --- 이후의 텍스트를 추출하는 함수\n",
    "def extract_reponse(text):\n",
    "    # --- 사이에 텍스트가 있는지 확인\n",
    "    pattern_01 = r'---\\n(.*?)\\n---'\n",
    "    matches_01 = re.findall(pattern_01, text, re.DOTALL)\n",
    "\n",
    "    # --- 이후 텍스트 추출\n",
    "    pattern_02 = r'---\\n(.*)'\n",
    "    matches_02 = re.search(pattern_02, text, re.DOTALL)\n",
    "\n",
    "    # 결과 반환\n",
    "    if matches_01:\n",
    "        result = matches_01[0]\n",
    "    elif matches_02:\n",
    "        result = matches_02.group(1)\n",
    "    elif '\\n' not in text: # 줄바꿈이 없는 경우\n",
    "        result = text\n",
    "    elif '\\n' in text: # 줄바꿈이 있는 경우\n",
    "        # ':' 뒤에 공백이 있으면 그 이전의 텍스트를 모두 제거함 (앞부분 사족은 대부분 이걸로 제거됨)-- 아이디어는 좋았는데, 다른 문장도 제거됨\n",
    "        # pattern_03 = r'.*:\\s+'\n",
    "        # matches_03 = re.sub(pattern_03, '', text, flags=re.DOTALL)\n",
    "\n",
    "        # 특정 문자열을 포함한 문장 제거를 위한 키워드 리스트\n",
    "        remove_keywords = [\"Sure,\", \"Here's the completion of the diary\", \"Here are your completed journal entries:\",\n",
    "                           \"2. \",\n",
    "                           \"I hope this helps\", \"Would you like to\", \"Does this help\", \"you would like to add or edit\", \"I hope this fills in the missing parts\",\n",
    "                           ]\n",
    "        # 텍스트를 문장 단위로 나누기\n",
    "        sentences = text.split('\\n')\n",
    "        # 여러 문자열 중 하나라도 포함된 문장 제거\n",
    "        filtered_sentences = [sentence for sentence in sentences if not any(keyword in sentence for keyword in remove_keywords)]\n",
    "        # 문장들을 다시 결합\n",
    "        result = ' '.join(filtered_sentences)\n",
    "    else:\n",
    "        result = None \n",
    "\n",
    "    # 잘못된 문법 정제\n",
    "    if result is not None:\n",
    "        result = result.strip().replace(\"?™\", \"'\").replace(\"I are\", \"I am\").replace(\"I were\", \"i am\").replace(\"1. \", \"\").strip('\"*')\n",
    "    \n",
    "    return result\n",
    "\n",
    "# apply()로 컬럼의 모든 데이터에 함수 적용\n",
    "data['Cleaned'] = data['gen_sentence'].apply(extract_reponse)\n",
    "\n",
    "# 결과 출력\n",
    "print(data['Cleaned'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 수 세기\n",
    "# 각 행의 텍스트에서 문장 수를 세는 함수 정의\n",
    "def count_sentences(text):\n",
    "    sentences = nltk.sent_tokenize(text)  # 문장을 토큰화\n",
    "    return len(sentences)  # 문장의 개수 반환\n",
    "\n",
    "# 'text_column'의 문장 수 계산하여 새로운 컬럼에 저장\n",
    "data['sentence_count'] = data['Cleaned'].apply(count_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_count\n",
       "5     2464\n",
       "6     2127\n",
       "4     1305\n",
       "7      992\n",
       "8      385\n",
       "9      134\n",
       "3       49\n",
       "10      27\n",
       "11       8\n",
       "12       3\n",
       "2        3\n",
       "13       1\n",
       "46       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentence_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_count\n",
       "5     2464\n",
       "6     2127\n",
       "4     1305\n",
       "7      992\n",
       "8      385\n",
       "9      134\n",
       "3       49\n",
       "10      27\n",
       "11       8\n",
       "12       3\n",
       "13       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence_count가 특정 숫자인 것 제거\n",
    "data = data[data['sentence_count'] != 2]\n",
    "data = data[data['sentence_count'] != 46]\n",
    "data['sentence_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While walking on the road, a stray dog rushes up and bites me. I scream in pain and try to push the dog away, but it's too quick. I must have rabies now, my family and friends must be very scared of me, and I don't know what to do.\n",
      "----------------\n",
      "While walking on the road, a stray dog rushes up and bites me. I scream in pain and try to push the dog away, but it's too quick. I must have rabies now, my family and friends must be very scared of me, and I don't know what to do.\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "n = 0\n",
    "# 검증\n",
    "print(data[data['sentence_count']==i]['gen_sentence'].iloc[n])\n",
    "print(\"----------------\")\n",
    "print(data[data['sentence_count']==i]['Cleaned'].iloc[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [scenario, thought, label, gen_sentence, base, long, similarity, average_similarity, Cleaned, sentence_count]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# 문장이 아예 없으면 안됨\n",
    "print(data[data['Cleaned'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is your completed diary entry including the provided sentences:\n",
      "\n",
      "I am not a good-looking child and I feel a little inferior. It's hard to feel confident when all the advertisements and media focus on perfect faces and bodies. The world only cares about beautiful people: I was born ugly, I will be ugly all my life, and I don't think I will ever become more beautiful. Sometimes, it's tough to find value in myself outside of appearance, but I know I have other qualities that matter. It's a daily struggle to remind myself that beauty is not everything.\n"
     ]
    }
   ],
   "source": [
    "# 확인\n",
    "# print(data['Cleaned'].iloc[87])\n",
    "print(data['gen_sentence'].iloc[87])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "data.to_csv(\"data/c2d2_after_gen_all_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gingerit.gingerit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgingerit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgingerit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GingerIt\n\u001b[0;32m      3\u001b[0m parser \u001b[38;5;241m=\u001b[39m GingerIt()\n\u001b[0;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis are a example of bad grammar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gingerit.gingerit'"
     ]
    }
   ],
   "source": [
    "# 문법 교정은 여러가지 라이브러리를 실행해보려 했는데, 설치가 제대로 되는 것이 없음\n",
    "# grammerly 사이트를 웹 크롤링으로 수정하는게 좋을 듯 하며, 작업이 크므로 나중으로 넘김"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotated data 에 맞춰 데이터셋을 변경함\n",
    "- 필요한 것은 아래와 같음\n",
    "1. stroy\n",
    "2. Distorted part\n",
    "3. Distorted type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "dt = pd.read_csv(\"data/c2d2_after_gen_all_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Are the people in this environment unfriendly?]\n",
       "1       [I'm so dizzy., Am I sick?, I should probably ...\n",
       "2       [I'm tired and there's no place to rest., I'm ...\n",
       "3       [Why did I catch a cold at this time?, I feel ...\n",
       "4       [Mom is trying to control my life again., want...\n",
       "                              ...                        \n",
       "7490    [Anyway., no matter how hard you work., you wo...\n",
       "7491    [I felt like I was being corrupted by somethin...\n",
       "7492    [These overtime jobs made me feel the darkness...\n",
       "7493    [As a newcomer., I had a feeling that I would ...\n",
       "7494    [Why didn't my roommates call me?, They must h...\n",
       "Name: Thought_Sentences, Length: 7495, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인지왜곡 생각을 인코딩 문제 보정 후, 문장 단위로 나누기\n",
    "dt['thought'] = dt['thought'].apply(ftfy.fix_text)\n",
    "\n",
    "dt['Thought_Sentences'] = dt['thought'].str.replace(', ', '. ').str.replace(\"?™\", \"'\").str.replace(\"I are\", \"I am\").str.replace(\"I were\", \"i am\")\n",
    "dt['Thought_Sentences'] = dt['Thought_Sentences'].apply(lambda x: nltk.sent_tokenize(x))\n",
    "dt['Thought_Sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "# 최종 문장에서 유사도 높은 문장 찾아내서 합침 (인지왜곡 문장인 Distorted part가 됨)\n",
    "\n",
    "result_df = pd.DataFrame() # 결과 저장할 데이터프레임 생성\n",
    "rmv_num = 0\n",
    "\n",
    "for i, row in dt.iterrows():\n",
    "    # 유사도 비교\n",
    "    base_list = []\n",
    "    long_list = []\n",
    "    similarity_list = []\n",
    "\n",
    "    gan_sentence = row['Cleaned']\n",
    "    # print(gan_sentence)\n",
    "\n",
    "    for base_text in row['Thought_Sentences']:\n",
    "        # print(base_text)\n",
    "        base, long, similarity = find_similar_sentences(base_text, gan_sentence)\n",
    "\n",
    "        base_list.append(base)\n",
    "        long_list.append(long)\n",
    "        similarity_list.append(similarity)\n",
    "\n",
    "    # 너무 정확도가 낮은 문장이 있다면 제외 (약 20개 제외됨)\n",
    "    has_below = any(num < 0.6 for num in similarity_list)\n",
    "    if has_below:\n",
    "        rmv_num += 1\n",
    "        continue        \n",
    "    \n",
    "    # long 텍스트 내 중복 제거\n",
    "    long_list = list(set(long_list))\n",
    "    \n",
    "    # 유사도가 있는 경우 평균 계산, 없으면 0으로 설정\n",
    "    if similarity_list:\n",
    "        average_similarity = sum(similarity_list) / len(similarity_list)\n",
    "    else:\n",
    "        average_similarity = 0\n",
    "        \n",
    "    # 데이터프레임으로 만들어서 비교\n",
    "    result = pd.DataFrame({\n",
    "        'story' : row['Cleaned'],\n",
    "        'Distorted part': [long_list],\n",
    "        'label' : row[\"label\"],\n",
    "        'Distorted_문장분리': [base_list],\n",
    "        'Distorted_문장유사도': [similarity_list],\n",
    "        'Distorted_문장유사도평균': average_similarity,\n",
    "        '기존데이터_scenario' : row[\"scenario\"],\n",
    "        '기존데이터_thought' : row[\"thought\"],\n",
    "    })\n",
    "\n",
    "    result_df = pd.concat([result_df, result], ignore_index=True)\n",
    "\n",
    "print(rmv_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "result_df.to_csv(f\"data/c2d2_0924_final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotated data 도 위 형식에 맞춰 약간 수정해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_Number</th>\n",
       "      <th>Patient Question</th>\n",
       "      <th>Distorted part</th>\n",
       "      <th>Dominant Distortion</th>\n",
       "      <th>Secondary Distortion (Optional)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4500</td>\n",
       "      <td>Hello, I have a beautiful,smart,outgoing and a...</td>\n",
       "      <td>The voice are always fimilar (someone she know...</td>\n",
       "      <td>Personalization</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id_Number                                   Patient Question  \\\n",
       "0       4500  Hello, I have a beautiful,smart,outgoing and a...   \n",
       "\n",
       "                                      Distorted part Dominant Distortion  \\\n",
       "0  The voice are always fimilar (someone she know...     Personalization   \n",
       "\n",
       "  Secondary Distortion (Optional)  \n",
       "0                             NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "dt = pd.read_csv(\"raw_data/Annotated_data.csv\")\n",
    "dt.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [The voice are always fimilar (someone she kno...\n",
       "1       [I feel trapped inside my disgusting self and ...\n",
       "2                                                        \n",
       "3                                                        \n",
       "4       [I refused to go because I didn’t know if it w...\n",
       "                              ...                        \n",
       "2525                                                     \n",
       "2526    [Now I am at university my peers around me all...\n",
       "2527    [He claims he’s severely depressed and has out...\n",
       "2528                                                     \n",
       "2529                                                     \n",
       "Name: Distorted_sen, Length: 2530, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 인지왜곡 생각을 인코딩 문제 보정 후, 문장 단위로 나누기\n",
    "\n",
    "# Distorted part가 비어 있거나 NaN인 경우 그대로 두고, 그렇지 않으면 문장 단위로 나누기\n",
    "dt['Distorted part'] = dt['Distorted part'].replace(np.nan, '')  # NaN을 빈 문자열로 치환\n",
    "\n",
    "dt['Distorted_sen'] = dt['Distorted part'].apply(lambda x: nltk.sent_tokenize(x) if isinstance(x, str) and x.strip() else x)\n",
    "dt['Distorted_sen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 최종 문장에서 유사도 높은 문장 찾아내서 합침 (인지왜곡 문장인 Distorted part가 됨)\n",
    "\n",
    "result_df = pd.DataFrame() # 결과 저장할 데이터프레임 생성\n",
    "rmv_num = 0\n",
    "\n",
    "for i, row in dt.iterrows():\n",
    "    # 유사도 비교\n",
    "    base_list = []\n",
    "    long_list = []\n",
    "    similarity_list = []\n",
    "\n",
    "    gan_sentence = row['Patient Question']\n",
    "    # print(gan_sentence)\n",
    "\n",
    "    for base_text in row['Distorted_sen']:\n",
    "        # print(base_text)\n",
    "        base, long, similarity = find_similar_sentences(base_text, gan_sentence)\n",
    "\n",
    "        base_list.append(base)\n",
    "        long_list.append(long)\n",
    "        similarity_list.append(similarity)\n",
    "\n",
    "    # 너무 정확도가 낮은 문장이 있다면 제외 (약 20개 제외됨)\n",
    "    has_below = any(num < 0.6 for num in similarity_list)\n",
    "    # if has_below:\n",
    "    #     rmv_num += 1\n",
    "    #     continue \n",
    "    \n",
    "    # long 텍스트 내 중복 제거\n",
    "    long_list = list(set(long_list))\n",
    "    \n",
    "    # 유사도가 있는 경우 평균 계산, 없으면 0으로 설정\n",
    "    if similarity_list:\n",
    "        average_similarity = sum(similarity_list) / len(similarity_list)\n",
    "    else:\n",
    "        average_similarity = 0\n",
    "        \n",
    "    # 데이터프레임으로 만들어서 비교\n",
    "    result = pd.DataFrame({\n",
    "        'story' : row['Patient Question'],\n",
    "        'Distorted part': [long_list],\n",
    "        'label' : row[\"Dominant Distortion\"],\n",
    "        'Distorted_문장분리': [base_list],\n",
    "        'Distorted_문장유사도': [similarity_list],\n",
    "        'Distorted_문장유사도평균': average_similarity,\n",
    "        '기존데이터_SecondaryDistortion' : row[\"Secondary Distortion (Optional)\"],\n",
    "    })\n",
    "\n",
    "    result_df = pd.concat([result_df, result], ignore_index=True)\n",
    "\n",
    "print(rmv_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "result_df.to_csv(f\"data/annotated_change.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
