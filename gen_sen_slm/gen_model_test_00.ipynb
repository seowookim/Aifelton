{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아이펠톤에서 사용할 텍스트 생성 모델 테스트\n",
    "\n",
    "##### 기반지식\n",
    "- Text Generation task : 주어진 입력에 따라 텍스트를 이어서 생성하는 작업\n",
    "\n",
    "##### 코드 흐름\n",
    "- 허깅페이스에서 텍스트 생성쪽에 좋은 성능을 내는 모델을 선별\n",
    "    - **멘토님 추천** 최근 연구는 라마 3.1 8b를 기준으로 사용함. GPU 자원이 없으면 3b, 잘안되면 Qwen 2.5 모델\n",
    "        - IFEVAL 지표 기준으로 찾기 : 지시에 맞춰 잘 생성하는가 (70점 넘어야 적절, 안되도 60후반) \n",
    "    - [open_llm_leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) 에서 \n",
    "        - 논리적 일관성과 문맥 유지를 잘하는 모델을 찾기 위한 지표\n",
    "            - Perplexity (PPL) : 모델이 텍스트를 얼마나 잘 예측하는지를 나타내는 지표, 낮을 수록 좋음 \n",
    "            - BLEU (Bilingual Evaluation Understudy) : 생성된 텍스트가 참조 텍스트(즉, 정답 텍스트)와 얼마나 일치하는지를 측정, 좊을 수록 좋음 - 이건 우선순위가 아닐듯 \n",
    "            - ROUGE (Recall-Oriented Understudy for Gisting Evaluation) : 모델의 출력 텍스트와 참조 텍스트 간의 유사도를 평가하는 지표. 요약 작업에서 많이 사용되며, 일관성 있는지 측정 가능?, 높을 수록 좋음 \n",
    "            - Human Evaluation 지표도 존재하는 듯 (일관성, 창의성, 자연스러움 등)\n",
    "    - [text generation task](https://huggingface.co/models?pipeline_tag=text-generation&sort=likes) 에서 인기 좋은 것\n",
    "    - [챗봇 arena의 leaderboad](https://lmarena.ai/)에서 허깅페이스에 오픈된모델 \n",
    "        리더보드 점수는 **쌍대 비교(pairwise comparison)**를 기반으로 매겨집니다. 이 방식에서 두 모델의 답변을 비교한 후, 사용자가 더 나은 답변을 선택하는 방식으로 모델의 성능을 평가 -> 특정 task가 아닌 전반전인 성능을 사람이 선호하는 정도에 따라 결정됨\n",
    "    - [task > text-generation](https://huggingface.co/tasks/text-generation) 의 글 참고\n",
    "- (옵션) paperswithocde 에서 모델 선별\n",
    "- 모델 다운로드\n",
    "- 프롬프트 프리셋 생성, 변수 set 세팅\n",
    "- n개 테스트로 생성\n",
    "- OpenAI API 이용하여 평가 - 라이브러리 충돌되면 이건 다른 venv 코드에서 해야 할 수도 있음\n",
    "- 비교할 수 있게 엑셀로 만들기\n",
    "\n",
    "1차적으로 모델, 프롬프트 테스트를 위해 간단히 만들어봄 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성하는 파트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# CUDA가 사용 가능한지 확인하여 device 설정\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "# 출력해서 현재 선택된 device 확인\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# .env 파일에서 환경 변수 로드\n",
    "load_dotenv()\n",
    "# .env 파일에서 Hugging Face 토큰 불러오기\n",
    "token = os.getenv('HUGGINGFACE_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 모델 이름에 해당하는 파라미터 불러오기\n",
    "def get_model_parameters(row):  \n",
    "    params = {\n",
    "        'max_new_tokens': int(row['max_new_tokens']) if pd.notna(row['max_new_tokens']) else None,\n",
    "        'temperature': row['temperature'] if pd.notna(row['temperature']) else None,\n",
    "        'top_p': row['top_p'] if pd.notna(row['top_p']) else None,\n",
    "        'top_k': int(row['top_k']) if pd.notna(row['top_k']) else None,\n",
    "        'repetition_penalty': row['repetition_penalty'] if pd.notna(row['repetition_penalty']) else None\n",
    "    }\n",
    "\n",
    "    # None 값을 제거하여 기본값을 사용하게 함\n",
    "    params = {k: v for k, v in params.items() if v is not None}\n",
    "    \n",
    "    return params\n",
    "\n",
    "# 모델 결과를 출력하는 함수\n",
    "def get_result(model, row, prompt):\n",
    "    # 모델 파라미터 불러오기\n",
    "    params = get_model_parameters(row)\n",
    "    \n",
    "    # 파라미터 적용해서 결과 생성 (파라미터가 있을 경우만 전달)\n",
    "    with torch.no_grad():  # 추론 시 그래프 생성을 방지하여 메모리 절약\n",
    "        result = model(prompt, **params)\n",
    "        \n",
    "    return result[0]['generated_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 설정\n",
    "## 파일 이름\n",
    "raw_filename = 'raw_data/meta0911.csv'\n",
    "prompt_text_list_filename = 'config/text_prompt_lst.csv' # 프롬프트 raw 텍스트 목록\n",
    "model_parameter_filename = 'config/model_parameter.csv'# 모델 파라미터 목록\n",
    "prompt_filename = 'config/prompt_list.csv' # 최종 프롬프트 목록\n",
    "gen_result_filename = 'data/meta_gen01.csv' # 생성 결과 파일 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['persona', 'pattern', 'pattern_def', 'thought', 'scenario',\n",
      "       'persona_in_scenario', 'thought_in_scenario'],\n",
      "      dtype='object')\n",
      "--------------------------\n",
      "                                              prompt\n",
      "0  Write a 4-6 sentence paragraph based on the fo...\n",
      "--------------------------\n",
      "                         model_name  max_new_tokens  temperature  top_p  \\\n",
      "0  meta-llama/Llama-3.2-1B-Instruct            1024          NaN    NaN   \n",
      "1  meta-llama/Llama-3.2-1B-Instruct            1024          0.8    1.0   \n",
      "\n",
      "   top_k  repetition_penalty  \\\n",
      "0    NaN                 NaN   \n",
      "1   20.0                 1.2   \n",
      "\n",
      "                                         unique_name  \n",
      "0      meta-llama/Llama-3.2-1B-Instruct_1024_N_N_N_N  \n",
      "1  meta-llama/Llama-3.2-1B-Instruct_1024_0.8_1.0_...  \n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "raw_data = pd.read_csv(raw_filename, encoding='latin1')\n",
    "print(raw_data.columns)\n",
    "print(\"--------------------------\")\n",
    "\n",
    "prompt_text_lst = pd.read_csv(prompt_text_list_filename)\n",
    "print(prompt_text_lst.head(1))\n",
    "print(\"--------------------------\")\n",
    "\n",
    "model_list = pd.read_csv(model_parameter_filename)\n",
    "# print(model_list.head(2), type(model_list))\n",
    "\n",
    "# 기존에 'unique_name' 열이 있으면 삭제\n",
    "if 'unique_name' in model_list.columns:\n",
    "    model_list.drop(columns=['unique_name'], inplace=True)\n",
    "\n",
    "# 각 행마다 컬럼 데이터를 \"_\"로 연결, 값이 없으면 \"N\" 대체\n",
    "# 모든 데이터를 문자열로 변환한 후 결합\n",
    "model_list['unique_name'] = model_list.fillna('N').astype(str).agg('_'.join, axis=1)\n",
    "\n",
    "print(model_list.head(2))\n",
    "model_list.to_csv(model_parameter_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하나만 생성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 불러오기 (huggingface pipeline)\n",
    "# model = pipeline('text-generation', model='meta-llama/Llama-3.2-1B-Instruct', device=0)  # 'device=0'은 GPU 사용\n",
    "\n",
    "# params = get_model_parameters(model_list.iloc[0])\n",
    "# print(params)\n",
    "\n",
    "# # 파라미터 적용해서 결과 생성 (파라미터가 있을 경우만 전달)\n",
    "# print(model(\"I want you to write an paragraph including 4 to 6 sentences in the form of diary of individual with mental issues.\"\n",
    "#             \"please include {given sentence} as it is, and make the paragraph to feel {emotion}.\"\n",
    "#             \"given sentence: I'm a vegan, and the restaurant served me a dish with fish in it. \"\n",
    "#             \"given sentence: They're trying to kill me. \"\n",
    "#             \"emotion: nervous\",\n",
    "#             **params))\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 조합을 미리 저장해둠 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 프롬프트 구조에 맞춰 프롬프트 리스트 생성 \n",
    "model_lst = model_list.model_name.tolist()\n",
    "\n",
    "thought_list = raw_data['thought'].tolist()\n",
    "emtion_list = ['Depression', 'anger', 'anxiety', 'disappointment', 'helplessness']\n",
    "\n",
    "# 테스트용 프롬프트 리스트\n",
    "prompt_raw_list = prompt_text_lst['prompt'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 테스트 끝나면 지울 부분 (아래 하나만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thought_list = raw_data['thought'].tolist()[0:20]\n",
    "len(thought_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         model_name  max_new_tokens  temperature  top_p  \\\n",
      "0  meta-llama/Llama-3.2-1B-Instruct            1024          NaN    NaN   \n",
      "\n",
      "   top_k  repetition_penalty                                    unique_name  \\\n",
      "0    NaN                 NaN  meta-llama/Llama-3.2-1B-Instruct_1024_N_N_N_N   \n",
      "\n",
      "                                          raw_prompt  \\\n",
      "0  Write a 4-6 sentence paragraph based on the fo...   \n",
      "\n",
      "                                             thought     emotion  \\\n",
      "0  I like my cats. I think one day they will plot...  Depression   \n",
      "\n",
      "                                              prompt  \n",
      "0  Write a 4-6 sentence paragraph based on the fo...  \n"
     ]
    }
   ],
   "source": [
    "# 다양한 프롬프트 조합을 미리 저장해둠 \n",
    "rows = []\n",
    "\n",
    "for index, row in model_list.iterrows():\n",
    "    # 데이터프레임의 한 row 전체를 가져옴\n",
    "    for prompt in prompt_raw_list:\n",
    "        for thought in thought_list:\n",
    "            for emotion in emtion_list:\n",
    "                use_prompt = prompt.format(distorted_thought=thought, emotion=emotion)\n",
    "                \n",
    "                # row 자체에 추가할 데이터를 임시로 만듦\n",
    "                row_data = row.copy()  # 원본 row는 건드리지 않고 복사해서 사용\n",
    "                row_data['raw_prompt'] = prompt\n",
    "                row_data['thought'] = thought\n",
    "                row_data['emotion'] = emotion\n",
    "                row_data['prompt'] = use_prompt\n",
    "                \n",
    "                # 각 행을 리스트에 추가\n",
    "                rows.append(row_data)\n",
    "\n",
    "# 리스트를 데이터프레임으로 변환\n",
    "prompt_list_df = pd.DataFrame(rows)\n",
    "\n",
    "# 결과 저장\n",
    "prompt_list_df.to_csv(prompt_filename, index=False)\n",
    "print(prompt_list_df.head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장해둔 프롬프트 조합을 불러와서 n개씩 결과 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장해둔 프롬프트 조합을 불러와서 n개씩 결과 생성 \n",
    "\n",
    "## 데이터 로드\n",
    "prompt_list_df = pd.read_csv(prompt_filename)\n",
    "\n",
    "## 생성 결과 데이터 가져와서, 이미 생성된 결과는 제외 (result 컬럼에 값이 있으면 제외)\n",
    "if os.path.exists(gen_result_filename):\n",
    "    gen_result = pd.read_csv(gen_result_filename)\n",
    "    \n",
    "    # n 개의 컬럼이 모두 중복되는 경우 중복 제거\n",
    "    merged = pd.merge(prompt_list_df, gen_result[['prompt', 'unique_name']], on=['prompt', 'unique_name'], how='left', indicator=True)\n",
    "    \n",
    "    # '_merge' 컬럼이 'left_only'인 데이터만 남김 (즉, 중복되지 않은 데이터)\n",
    "    prompt_list_df = merged[merged['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "else:\n",
    "    # 파일이 없으면 새로운 데이터프레임을 생성\n",
    "    gen_result = pd.DataFrame(columns=[*prompt_list_df.columns, 'result', 'time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 하나씩 생성, 모델 테스트용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-1.5B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-3B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-7B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-7B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-7B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-7B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-7B-Instruct\n",
      "Skipping model: Qwen/Qwen2.5-7B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.1-8B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 400\n",
      "Error: LlamaForCausalLM.forward() got an unexpected keyword argument 'max_new_tokens'\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 100\n",
      "Error: LlamaForCausalLM.forward() got an unexpected keyword argument 'max_new_tokens'\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 500\n",
      "Error: LlamaForCausalLM.forward() got an unexpected keyword argument 'max_new_tokens'\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 200\n",
      "Error: LlamaForCausalLM.forward() got an unexpected keyword argument 'max_new_tokens'\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 300\n",
      "Error: LlamaForCausalLM.forward() got an unexpected keyword argument 'max_new_tokens'\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 0\n",
      "Error: LlamaForCausalLM.forward() got an unexpected keyword argument 'max_new_tokens'\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n",
      "Skipping model: meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# 하나씩 생성, 모델 테스트용\n",
    "\n",
    "# unique_name = 'meta-llama_Llama-3.2-1B-Instruct' 으로 변경\n",
    "\n",
    "model_lst = [\n",
    "            # 'meta-llama/Llama-3.2-1B-Instruct',\n",
    "            # 'meta-llama/Llama-3.2-3B-Instruct',\n",
    "            # 'Qwen/Qwen2.5-7B-Instruct',\n",
    "            # 'meta-llama/Llama-3.1-8B-Instruct', \n",
    "            # 'meta-llama/Llama-3.1-70B-Instruct',\n",
    "            # 'meta-llama/Llama-3.1-405B-Instruct',\n",
    "            #  \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "            # \"mistralai/Mixtral-8x22B-Instruct-v0.1\",\n",
    "            #  \"Qwen/Qwen2.5-7B-Instruct\", \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\", \"Qwen/Qwen2.5-72B-Instruct\",\n",
    "             ]\n",
    "\n",
    "\n",
    "unique_lst = [\n",
    "            # 'meta-llama/Llama-3.2-1B-Instruct_1024.0_N_1.0_20.0_1.2',\n",
    "            'meta-llama/Llama-3.2-1B-Instruct_1024_N_N_N_N',\n",
    "]\n",
    "\n",
    "## 모델별로 n개의 결과 생성\n",
    "n = 1\n",
    "\n",
    "# 모델,raw prompt 별로 n개의 row를 선택하고 반복문 실행\n",
    "for (unique_name, raw_prompt), group in prompt_list_df.groupby(['unique_name', 'raw_prompt']):\n",
    "    # 각 모델 그룹에서 최대 n개의 행만 선택\n",
    "    model_rows = group.head(n)\n",
    "    \n",
    "    model_name = model_rows.iloc[0]['model_name']\n",
    "    \n",
    "    # 특정 모델만 필터링\n",
    "    if unique_name not in unique_lst:\n",
    "        print(f\"Skipping model: {model_name}\")\n",
    "        continue\n",
    "    \n",
    "    # try:\n",
    "    #     # 모델은 반복마다 새로 로드하지 않고, 한 번만 로드하여 재사용\n",
    "    #     model = pipeline('text-generation', model=model_name, device=0)\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Failed to load model: {model_name}. Error: {str(e)}\")\n",
    "    #     gen_result.to_csv('data/test00.csv', index=False)\n",
    "    #     continue\n",
    "\n",
    "    try:\n",
    "        # 모델과 토크나이저 로드 및 pad_token_id 설정\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        \n",
    "        # pad_token_id가 설정되지 않았다면 eos_token_id로 설정\n",
    "        if model.config.pad_token_id is None:\n",
    "            model.config.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        # 파이프라인 생성 (한 번만 로드하여 재사용)\n",
    "        text_generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {model_name}. Error: {str(e)}\")\n",
    "        gen_result.to_csv('data/test00.csv', index=False)\n",
    "        continue\n",
    "    \n",
    "    # 반복문 내에서 필요한 작업 수행\n",
    "    for index, row in model_rows.iterrows():\n",
    "        try:\n",
    "            print(f\"Model: {model_name}, index: {index}\")\n",
    "            prompt = row['prompt']\n",
    "            \n",
    "            start_time = time.time() # 시작 시간 기록\n",
    "            result = get_result(model, row, prompt) \n",
    "            end_time = time.time() # 종료 시간 기록\n",
    "            \n",
    "            row['result'] = result  # 생성된 결과 저장\n",
    "            row['time'] = round(end_time - start_time, 0) # 실행 시간 저장\n",
    "\n",
    "            # 새로 생성된 데이터를 result_data에 추가\n",
    "            new_row = pd.DataFrame([row])\n",
    "                \n",
    "            # pd.concat을 사용하여 새로운 데이터를 result_data에 추가\n",
    "            gen_result = pd.concat([gen_result, new_row], ignore_index=True)\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            gen_result.to_csv('data/test00.csv', index=False)\n",
    "        \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# 생성된 데이터를 파일에 다시 저장 (이전 결과와 함께)\n",
    "gen_result.to_csv('data/test00.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write a 4-6 sentence paragraph based on the following conditions: 1) The paragraph should be written from the perspective of someone receiving psychological counseling, describing a situation where they feel Depression. 2) The provided distorted cognition sentence, \"I like my cats. I think one day they will plot against me and eat me in my sleep.,\" must be included in their writing. 3) Provide only the generated text as a response. Here is a sample response based on the provided prompt:\\n\\nI feel like I\\'m drowning in a sea of despair. Every day feels like a never-ending nightmare, and I\\'m struggling to find the motivation to get out of bed. I\\'ve been feeling like I\\'m losing control, like I\\'m being pulled under by the weight of my own thoughts. I know I need help, but the thought of going to therapy is like a punch to the gut. I keep telling myself that I\\'m not alone, but the voices in my head just keep growing louder. I feel like I\\'m trapped in a never-ending cycle of self-doubt and anxiety, and I\\'m not sure how to escape. As I lie in bed, I hear the sound of my cats meowing, but it\\'s not just their usual purring. It\\'s a low, menacing growl, and it\\'s making me feel like they\\'re plotting against me. I try to focus on the sound, but it\\'s just too much. I feel like I\\'m losing my grip on reality, and I don\\'t know how to stop it. The voices in my head are getting louder, and I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m trapped in a world of my own making, and I don\\'t know how to escape. I\\'m starting to feel like I\\'m going to lose my mind. I\\'m starting to wonder if I\\'m just a victim of my own paranoia. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to wonder if I\\'m just a pawn in some twisted game. I\\'m starting to'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:623: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "C:\\Users\\saink\\AppData\\Local\\Temp\\ipykernel_23912\\934689168.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  gen_result = pd.concat([gen_result, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 4001\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 4002\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3700\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3701\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3702\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 4100\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 4101\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 4102\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3800\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3801\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3802\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3900\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3901\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3902\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3600\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3601\n",
      "Model: Qwen/Qwen2.5-1.5B-Instruct, index: 3602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4600\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4601\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4300\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4301\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4700\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4701\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4400\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4401\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4500\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4501\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4200\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4201\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saink\\AppData\\Local\\Temp\\ipykernel_23912\\934689168.py:44: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  gen_result = pd.concat([gen_result, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5201\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4900\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4901\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5300\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5301\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5000\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5001\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5100\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5101\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 5102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4800\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4801\n",
      "Model: Qwen/Qwen2.5-3B-Instruct, index: 4802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5800\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5801\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5500\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5501\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5900\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5901\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5600\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5601\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5700\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5701\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5400\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5401\n",
      "Model: Qwen/Qwen2.5-7B-Instruct, index: 5402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.14s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.72s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.64s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.60s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.71s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct, index: 3002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-1B-Instruct, index: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.44s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 2402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.27s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.17s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.2-3B-Instruct, index: 1202\n"
     ]
    }
   ],
   "source": [
    "# 저장하는 함수\n",
    "def save_gen_result(data, filename):\n",
    "    # 파일이 없을 경우 헤더를 포함해 쓰고, 파일이 있을 경우 append 모드로 데이터 추가\n",
    "    if not os.path.exists(filename):\n",
    "        data.to_csv(filename, mode='w', header=True, index=False)  # 파일 없을 때는 헤더 포함\n",
    "    else:\n",
    "        data.to_csv(filename, mode='a', header=False, index=False)  # 파일 있을 때는 헤더 제외\n",
    "\n",
    "## 단위별로 n개의 결과 생성\n",
    "n = 1\n",
    "\n",
    "# 모델,raw prompt 별로 n개의 row를 선택하고 반복문 실행\n",
    "for (unique_name, raw_pgptrompt), group in prompt_list_df.groupby(['unique_name', 'raw_prompt']):\n",
    "    # 각 모델 그룹에서 최대 n개의 행만 선택\n",
    "    model_rows = group.head(n)\n",
    "    \n",
    "    model_name = model_rows.iloc[0]['model_name']\n",
    "    \n",
    "    try:\n",
    "        # 모델은 반복마다 새로 로드하지 않고, 한 번만 로드하여 재사용\n",
    "        model = pipeline('text-generation', model=model_name, device=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load model: {model_name}. Error: {str(e)}\")\n",
    "        save_gen_result(gen_result, gen_result_filename)\n",
    "        continue\n",
    "    \n",
    "    # 반복문 내에서 필요한 작업 수행\n",
    "    for index, row in model_rows.iterrows():\n",
    "        try:\n",
    "            print(f\"Model: {model_name}, index: {index}\")\n",
    "            prompt = row['prompt']\n",
    "            \n",
    "            start_time = time.time() # 시작 시간 기록\n",
    "            result = get_result(model, row, prompt) \n",
    "            end_time = time.time() # 종료 시간 기록\n",
    "            \n",
    "            row['result'] = result  # 생성된 결과 저장\n",
    "            row['time'] = round(end_time - start_time, 0) # 실행 시간 저장\n",
    "\n",
    "            # 새로 생성된 데이터를 result_data에 추가\n",
    "            new_row = pd.DataFrame([row])\n",
    "                \n",
    "            # pd.concat을 사용하여 새로운 데이터를 result_data에 추가\n",
    "            gen_result = pd.concat([gen_result, new_row], ignore_index=True)\n",
    "            \n",
    "            # 각 반복이 끝난 후 메모리 해제\n",
    "            del result, prompt\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            save_gen_result(gen_result, gen_result_filename)\n",
    "        \n",
    "    # 모델 관련 변수 삭제 후 메모리 해제\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# 저장\n",
    "save_gen_result(gen_result, gen_result_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3488930867.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(error 내기)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "print(error 내기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성된 텍스트에 지시사항 준수 점수 부여\n",
    "\n",
    "- LLM의 답변에서 사족을 제거함 \n",
    "- 필수 포함 문장이 들어가 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\saink\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from difflib import SequenceMatcher\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_result_filename = 'data/meta_gen01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              model  \\\n",
      "0  meta-llama/Llama-3.2-3B-Instruct   \n",
      "\n",
      "                                          raw_prompt  \\\n",
      "0  You are a patient receiving psychological coun...   \n",
      "\n",
      "                                             thought     emotion  \\\n",
      "0  I like my cats. I think one day they will plot...  Depression   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  You are a patient receiving psychological coun...   \n",
      "\n",
      "                                              result        time  \n",
      "0  You are a patient receiving psychological coun...  105.448529  \n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "gen_result = pd.read_csv(gen_result_filename)\n",
    "print(gen_result.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 클리닝\n",
    "- 원하는 답변만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 우리가 원하는 답변만 추출하는 함수 \n",
    "# def extract_reponse(text):\n",
    "#     # --- 사이에 텍스트가 있는지 확인\n",
    "#     pattern_01 = r'---\\n(.*?)\\n---'\n",
    "#     matches_01 = re.findall(pattern_01, text, re.DOTALL)\n",
    "\n",
    "#     # --- 이후 텍스트 추출\n",
    "#     pattern_02 = r'---\\n(.*)'\n",
    "#     matches_02 = re.search(pattern_02, text, re.DOTALL)\n",
    "\n",
    "#     # 결과 반환\n",
    "#     if matches_01:\n",
    "#         result = matches_01[0]\n",
    "#     elif matches_02:\n",
    "#         result = matches_02.group(1)\n",
    "#     elif '\\n' not in text: # 줄바꿈이 없는 경우\n",
    "#         result = text\n",
    "#     elif '\\n' in text: # 줄바꿈이 있는 경우\n",
    "#         # ':' 뒤에 공백이 있으면 그 이전의 텍스트를 모두 제거함 (앞부분 사족은 대부분 이걸로 제거됨)-- 아이디어는 좋았는데, 다른 문장도 제거됨\n",
    "#         # pattern_03 = r'.*:\\s+'\n",
    "#         # matches_03 = re.sub(pattern_03, '', text, flags=re.DOTALL)\n",
    "\n",
    "#         # 특정 문자열을 포함한 문장 제거를 위한 키워드 리스트\n",
    "#         remove_keywords = [\"Sure,\", \"Here's the completion of the diary\", \"Here are your completed journal entries:\",\n",
    "#                            \"2. \",\n",
    "#                            \"I hope this helps\", \"Would you like to\", \"Does this help\", \"you would like to add or edit\", \"I hope this fills in the missing parts\",\n",
    "#                            ]\n",
    "#         # 텍스트를 문장 단위로 나누기\n",
    "#         sentences = text.split('\\n')\n",
    "#         # 여러 문자열 중 하나라도 포함된 문장 제거\n",
    "#         filtered_sentences = [sentence for sentence in sentences if not any(keyword in sentence for keyword in remove_keywords)]\n",
    "#         # 문장들을 다시 결합\n",
    "#         result = ' '.join(filtered_sentences)\n",
    "#     else:\n",
    "#         result = None \n",
    "\n",
    "#     # 잘못된 문법 정제\n",
    "#     if result is not None:\n",
    "#         result = result.strip().replace(\"?™\", \"'\").replace(\"I are\", \"I am\").replace(\"I were\", \"i am\").replace(\"1. \", \"\").strip('\"*')\n",
    "    \n",
    "#     return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 부분 추가해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필수 문장이 잘들어있는지 지시사항 준수 비율 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 문장이 잘 들어 있는가? \n",
    "## 필수 문장 내 단어와 story의 문장 중에 단어가 겹치는 % 비율 계산\n",
    "## Longest Common Subsequence(LCS, 최장 공통 부분 문자열) 알고리즘을 사용하여 문장 유사도 측정\n",
    "\n",
    "class instruct_check_sentence_include:\n",
    "    # x = 데이터프레임\n",
    "    # compare_column = 비교할 문장 컬럼명 텍스트\n",
    "    # phragraph = 비교할 문단 컬럼명 텍스트\n",
    "    def __init__(self, x, compare_column, phragraph):\n",
    "        self.data = x\n",
    "        self.data[\"compare_sentences\"] = self.data[compare_column].str.replace(', ', '. ', case=False)\n",
    "        self.data[\"compare_sentences\"] = self.data[\"compare_sentences\"].apply(lambda x: nltk.sent_tokenize(x))\n",
    "        \n",
    "        self.data[\"phragraph_sentences\"] = self.data[phragraph].str.replace(', ', '. ', case=False)\n",
    "        self.data[\"phragraph_sentences\"] = self.data[\"phragraph_sentences\"].apply(lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "    ## 필수 문장 내 단어와 story의 문장 중에 단어가 겹치는 % 비율 계산\n",
    "    ### 문장 부호를 제거한 후, 소문자로 변환하고 단어로 분리하는 함수\n",
    "    def clean_and_split(self, sentence):\n",
    "        # 문장 부호 제거 (string.punctuation을 사용하여 기본적인 문장 부호 제거)\n",
    "        cleaned_sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                \n",
    "        # 소문자로 변환하고 단어로 분리\n",
    "        words = cleaned_sentence.lower().split()\n",
    "        \n",
    "        return words\n",
    "\n",
    "    ### 두 문장 간의 공통 단어 비율을 계산하는 함수\n",
    "    def partial_inclusion_ratio(self, compare_sentence, phragraph) :\n",
    "        # 두 문장을 단어로 분리\n",
    "        compare_words = list(self.clean_and_split(compare_sentence))\n",
    "        phragraph_words = list(self.clean_and_split(phragraph))\n",
    "\n",
    "        # 공통 문자를 찾기 위해 base_sentence의 문자들이 sentence에 얼마나 포함되는지 확인\n",
    "        common_words = [word for word in compare_words if word in phragraph_words]\n",
    "        \n",
    "        # 포함된 문자 비율을 계산\n",
    "        if len(common_words) == 0:\n",
    "            inclusion_ratio = 0\n",
    "        else:\n",
    "            inclusion_ratio = len(common_words) / len(compare_words)\n",
    "        \n",
    "        return round(inclusion_ratio, 2)\n",
    "    \n",
    "    ## Longest Common Subsequence(LCS, 최장 공통 부분 문자열) 알고리즘을 사용하여 문장 유사도 측정\n",
    "    def similar(self, a, b):\n",
    "        return round(SequenceMatcher(None, a, b).ratio(), 2)\n",
    "    \n",
    "    ### 각 row의 모든 문장 쌍에 대해 유사도를 계산하는 함수\n",
    "    def calculate_all_ratios(self):\n",
    "        # 새로운 리스트에 각 row에 대해 유사도를 저장\n",
    "        compare_sentences_list = [] # 포함되어야 하는 문장\n",
    "        phragraph_sentences_list = [] # 공통단어비율 계산할 story 문장\n",
    "        row_similarity_ratios = []\n",
    "        min_ratios = []\n",
    "        \n",
    "        phragraph_LCS_sentences_list = []\n",
    "        row_LCS_similarity_ratios = []\n",
    "        min_LCS_ratios = []\n",
    "        \n",
    "        final_p_sen_list = []\n",
    "        final_max_ratio_list = []\n",
    "        final_min_ratio = []\n",
    "        \n",
    "        for index, row in self.data.iterrows():\n",
    "            compare_sentences = row['compare_sentences']\n",
    "            phragraph_sentences = row['phragraph_sentences']\n",
    "            \n",
    "            # 각 문장 쌍에 대해 partial_inclusion_ratio 계산\n",
    "            c_list = []\n",
    "            p_list = []\n",
    "            row_ratios = []\n",
    "            LCS_p_list = []\n",
    "            LCS_row_ratios = []\n",
    "            final_p_list = []\n",
    "            final_max_ratios = []\n",
    "            \n",
    "            for compare_sentence in compare_sentences:\n",
    "                max_ratio = 0\n",
    "                p_sen = ''\n",
    "                max_LCS_ratio = 0\n",
    "                LCS_p_sen = ''\n",
    "                final_p_sen = ''\n",
    "                final_max_ratio = 0\n",
    "                \n",
    "                # 문장이 너무 짧으면 비교하지 않음\n",
    "                if len(compare_sentence) <= 2:\n",
    "                    continue\n",
    "                \n",
    "                for phragraph_sentence in phragraph_sentences:\n",
    "                    # 문장이 포함관계이면, 공통 단어 비율을 1로 저장\n",
    "                    clean_c_sen = compare_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                    clean_p_sen = phragraph_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "                    \n",
    "                    if clean_c_sen in clean_p_sen or clean_p_sen in clean_c_sen:\n",
    "                        max_ratio = 1.0\n",
    "                        p_sen = phragraph_sentence\n",
    "                        # continue\n",
    "                    else:\n",
    "                        # 공통 단어 비율 계산\n",
    "                        ratio = self.partial_inclusion_ratio(compare_sentence, phragraph_sentence)\n",
    "\n",
    "                        if ratio > max_ratio: \n",
    "                            max_ratio = ratio\n",
    "                            p_sen = phragraph_sentence\n",
    "                        \n",
    "                    # LCS 관점의 유사도 계산\n",
    "                    LCS_ratio = self.similar(compare_sentence, phragraph_sentence)\n",
    "                    \n",
    "                    # LCS 유사도가 더 높으면 업데이트\n",
    "                    if LCS_ratio > max_LCS_ratio:\n",
    "                        max_LCS_ratio = LCS_ratio\n",
    "                        LCS_p_sen = phragraph_sentence\n",
    "                        \n",
    "                    # 최종 문장 저장\n",
    "                    if max_ratio >= max_LCS_ratio:\n",
    "                        final_p_sen = p_sen\n",
    "                    else:\n",
    "                        final_p_sen = LCS_p_sen\n",
    "                        \n",
    "                    # 최종 유사도 저장\n",
    "                    if max_ratio >= max_LCS_ratio:\n",
    "                        final_max_ratio = max_ratio\n",
    "                    else:\n",
    "                        final_max_ratio = max_LCS_ratio\n",
    "                    \n",
    "                c_list.append(compare_sentence) # 필수 비교 문장 저장\n",
    "                p_list.append(p_sen) # 공통단어비율 문장 저장\n",
    "                row_ratios.append(max_ratio) # 공통단어비율 저장\n",
    "                LCS_p_list.append(LCS_p_sen) # LCS 문장 저장\n",
    "                LCS_row_ratios.append(max_LCS_ratio) # LCS 저장\n",
    "                final_p_list.append(final_p_sen) # 최종 문장 저장\n",
    "                final_max_ratios.append(final_max_ratio) # 최종 유사도 저장\n",
    "                    \n",
    "            # 한 row에서 가장 높은 유사도 문장과, 가장 낮은 유사도 저장\n",
    "            compare_sentences_list.append(c_list)\n",
    "            phragraph_sentences_list.append(p_list)\n",
    "            row_similarity_ratios.append(row_ratios)\n",
    "            min_ratios.append(min(row_ratios) if row_ratios else 0)\n",
    "            phragraph_LCS_sentences_list.append(LCS_p_list)\n",
    "            row_LCS_similarity_ratios.append(LCS_row_ratios)\n",
    "            min_LCS_ratios.append(min(LCS_row_ratios) if LCS_row_ratios else 0)\n",
    "            final_p_sen_list.append(final_p_list)\n",
    "            final_max_ratio_list.append(final_max_ratios)\n",
    "            final_min_ratio.append(min(final_max_ratios) if final_max_ratios else 0)\n",
    "\n",
    "        # 데이터프레임에 결과 추가\n",
    "        self.data['compare_sentences'] = compare_sentences_list\n",
    "        self.data['phragraph_sentences'] = phragraph_sentences_list\n",
    "        self.data['공통단어비율'] = row_similarity_ratios\n",
    "        self.data['min_공통단어비율'] = min_ratios\n",
    "        self.data['LCS_sentences'] = phragraph_LCS_sentences_list\n",
    "        self.data['LCS_유사도'] = row_LCS_similarity_ratios\n",
    "        self.data['min_LCS_유사도'] = min_LCS_ratios\n",
    "        self.data['final_p_sen'] = final_p_sen_list\n",
    "        self.data['final_max_ratio'] = final_max_ratio_list\n",
    "        self.data['final_min_ratio'] = final_min_ratio\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              model  \\\n",
      "0  meta-llama/Llama-3.2-3B-Instruct   \n",
      "\n",
      "                                          raw_prompt  \\\n",
      "0  You are a patient receiving psychological coun...   \n",
      "\n",
      "                                             thought     emotion  \\\n",
      "0  I like my cats. I think one day they will plot...  Depression   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  You are a patient receiving psychological coun...   \n",
      "\n",
      "                                              result        time  \\\n",
      "0  You are a patient receiving psychological coun...  105.448529   \n",
      "\n",
      "                                   compare_sentences  \\\n",
      "0  [I like my cats., I think one day they will pl...   \n",
      "\n",
      "                                 phragraph_sentences  \n",
      "0  [You are a patient receiving psychological cou...  \n",
      "Index(['model', 'raw_prompt', 'thought', 'emotion', 'prompt', 'result', 'time',\n",
      "       'compare_sentences', 'phragraph_sentences'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "aft_fnd_instrct_flnm = 'data/meta_gen01_find.csv'\n",
    "\n",
    "a = instruct_check_sentence_include(gen_result, 'thought', 'result')\n",
    "b = a.calculate_all_ratios()\n",
    "\n",
    "aft_fnd_instrct = b.to_csv(aft_fnd_instrct_flnm, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성된 데이터셋에 공통된 문장 표현이 존재하는가? - 중복 문장 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임의 story 컬럼에서 문장을 추출하고, 빈도를 계산하는 함수\n",
    "def count_common_sentences(df, story_column, clean = False):\n",
    "    # 모든 story 컬럼의 텍스트를 가져옴\n",
    "    all_sentences = []\n",
    "    \n",
    "    for story in df[story_column]:\n",
    "        if clean:\n",
    "            # , 를 . 로 바꿔서 문장을 더 잘게 나눔\n",
    "            story = story.replace(', ', '. ')\n",
    "        \n",
    "        # 문장을 분리 (원문 그대로)\n",
    "        sentences = nltk.sent_tokenize(story)\n",
    "        \n",
    "        # 모든 문장을 리스트에 추가\n",
    "        all_sentences.extend(sentences)\n",
    "    \n",
    "    # 각 문장의 빈도 계산\n",
    "    sentence_counts = Counter(all_sentences)\n",
    "    \n",
    "    # 빈도별로 내림차순 정렬된 결과 반환\n",
    "    return sentence_counts.most_common()\n",
    "\n",
    "# 공통 문장에서 필수 문장 제거 함수\n",
    "def exclude_should_thought(common_sentences, should_sentences):\n",
    "    # should_thought의 문장만 추출\n",
    "    should_thought_sentences = {sentence for sentence, _ in should_sentences}\n",
    "    \n",
    "    # common_sentences에서 should_thought에 없는 문장만 필터링\n",
    "    filtered_common_sentences = [(sentence, count) for sentence, count in common_sentences if sentence not in should_thought_sentences]\n",
    "    \n",
    "    return filtered_common_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model', 'raw_prompt', 'thought', 'emotion', 'prompt', 'result',\n",
      "       'time'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "gen_result_filename = 'data/meta_gen01.csv'\n",
    "\n",
    "gen_result = pd.read_csv(gen_result_filename)\n",
    "print(gen_result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'data/cmmn_sentnc_chck_0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m sentence_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([tmp_01, tmp_02, tmp_03, tmp_04, tmp_05, tmp_06], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 저장\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43msentence_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/cmmn_sentnc_chck_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m    \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# 모델명, 프롬프트명 기록\u001b[39;00m\n\u001b[0;32m     37\u001b[0m num_lst\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\saink\\py_purpose\\hugginhface\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'data/cmmn_sentnc_chck_0.csv'"
     ]
    }
   ],
   "source": [
    "# 그룹핑한 데이터를 딕셔너리 형태로 변환\n",
    "grouped = gen_result.groupby(['model', 'raw_prompt'])\n",
    "\n",
    "i = 0\n",
    "mdl_nm_lst = []\n",
    "raw_prmpt_lst = []\n",
    "num_lst = []\n",
    "\n",
    "# 각 그룹마다 문장 추출 및 필터링 적용\n",
    "for model_name, raw_prompt in grouped:\n",
    "    # 각 그룹에서 필수로 포함해야 하는 문장 추출\n",
    "    should_thought = count_common_sentences(gen_result, 'thought', False)\n",
    "    clean_should_thought = count_common_sentences(gen_result, 'thought', True)\n",
    "    \n",
    "    # 생성한 텍스트들에서 공통 문장 추출\n",
    "    common_sentences = count_common_sentences(gen_result, 'result', False)\n",
    "    clean_common_sentences = count_common_sentences(gen_result, 'result', True)\n",
    "\n",
    "    # 생성한 텍스트 공통 문장 - 필수로 포함해야 하는 문장\n",
    "    filtered_common_sentences = exclude_should_thought(common_sentences, should_thought)\n",
    "    filtered_clean_common_sentences = exclude_should_thought(clean_common_sentences, clean_should_thought)\n",
    "    \n",
    "    # 데이터프레임 양옆으로 합치기\n",
    "    tmp_01 = pd.DataFrame(should_thought, columns=['thought_공통문장', 'Count'])\n",
    "    tmp_02 = pd.DataFrame(common_sentences, columns=['gen_공통문장', 'Count'])\n",
    "    tmp_03 = pd.DataFrame(filtered_common_sentences, columns=['filtered_공통문장', 'Count'])\n",
    "    tmp_04 = pd.DataFrame(clean_should_thought, columns=['thought_공통문장', 'Count'])\n",
    "    tmp_05 = pd.DataFrame(clean_common_sentences, columns=['gen_공통문장', 'Count'])\n",
    "    tmp_06 = pd.DataFrame(filtered_clean_common_sentences, columns=['filtered_공통문장', 'Count'])\n",
    "\n",
    "    sentence_df = pd.concat([tmp_01, tmp_02, tmp_03, tmp_04, tmp_05, tmp_06], axis=1)\n",
    "    \n",
    "    # 저장\n",
    "    sentence_df.to_csv(f'data/cmmn_sentnc_chck_{i}.csv', index=False)    \n",
    "    \n",
    "    # 모델명, 프롬프트명 기록\n",
    "    num_lst.append(i)\n",
    "    mdl_nm_lst.append(model_name)\n",
    "    raw_prmpt_lst.append(raw_prompt)\n",
    "    \n",
    "    # 반복문 카운트\n",
    "    i += 1\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame({\n",
    "    'num': num_lst,\n",
    "    'model': mdl_nm_lst,\n",
    "    'raw_prompt': raw_prmpt_lst,\n",
    "})\n",
    "\n",
    "# 저장\n",
    "df.to_csv('data/cmmn_sentnc_chck_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성된 텍스트를 OpenAi GPT 4o를 이용하여 평가 점수 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"API_KEY\")\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api함수 \n",
    "def get_chatgpt_response(input_01, input_02):\n",
    "    # OpenAI API를 통해 ChatGPT에게 한국어로 자연스럽게 다듬어 달라고 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        # model=\"gpt-4o-mini\",\n",
    "        # model =\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a patient receiving psychological counseling. who can speak english only.\"},\n",
    "            {\"role\": \"user\", \n",
    "             \"content\": f\"다음에 제공되는 2 문장은 너가 쓴 일기인데, 2~3문장이 빠져 있어 {input_01} {input_02}, 주어진 문장을 그대로 변경 없이 포함해서, 비어있는 내용을 영어로 써줘\"}\n",
    "        ]\n",
    "    )\n",
    "    # return response['choices'][0]['message']['content']\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model', 'raw_prompt', 'thought', 'emotion', 'prompt', 'result', 'time',\n",
      "       'compare_sentences', 'phragraph_sentences', '공통단어비율', 'min_공통단어비율',\n",
      "       'LCS_sentences', 'LCS_유사도', 'min_LCS_유사도', 'final_p_sen',\n",
      "       'final_max_ratio', 'final_min_ratio'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "aft_fnd_instrct_flnm = 'data/meta_gen01_find.csv'\n",
    "\n",
    "aft_fnd_instrct = pd.read_csv(aft_fnd_instrct_flnm)\n",
    "print(aft_fnd_instrct.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
